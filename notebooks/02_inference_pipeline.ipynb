{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inference Pipeline and Format Selection (Course 2)\n",
        "\n",
        "This notebook demonstrates model format selection and the inference pipeline. Inspired by [Intro to Inference: How to Run AI Models on a GPU](https://developers.google.com/learn/pathways/ai-models-on-gpu-intro).\n",
        "\n",
        "**Colab:** Run the setup cell first. **Local:** Ensure you are in the project root or have run notebook 01."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Colab: clone and install (skip locally)\n",
        "try:\n",
        "    import google.colab\n",
        "    get_ipython().system(\"git clone -q https://github.com/KarthikSriramGit/Project-Insight.git\")\n",
        "    get_ipython().run_line_magic(\"cd\", \"Project-Insight\")\n",
        "    get_ipython().system(\"pip install -q transformers torch\")\n",
        "except Exception:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROOT=C:\\Users\\skart\\Desktop\\ROG SSD\\Git Repos\\Project-Insight\n"
          ]
        }
      ],
      "source": [
        "# Setup: works from Colab (repo root) or local (notebooks/)\n",
        "import sys\n",
        "from pathlib import Path\n",
        "ROOT = Path(\".\").resolve() if (Path(\".\") / \"data\").exists() else Path(\"..\").resolve()\n",
        "if str(ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(ROOT))\n",
        "print(f\"ROOT={ROOT}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Format selection by use case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "research: safetensors\n",
            "  Fast, secure weight serialization. Memory-mapped loading, no arbitrary code execution...\n",
            "\n",
            "sharing: safetensors\n",
            "  Fast, secure weight serialization. Memory-mapped loading, no arbitrary code execution...\n",
            "\n",
            "local: gguf\n",
            "  Compact, quantized format for local inference. Powers llama.cpp and run-on-laptop wor...\n",
            "\n",
            "production: tensorrt\n",
            "  Compiled engine for NVIDIA GPUs. Pre-optimized kernels, lowest latency and highest th...\n",
            "\n",
            "portable: onnx\n",
            "  Graph-level interchange format. Framework-agnostic, runs on ONNX Runtime, OpenVINO, T...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from src.inference.format_selector import select_format\n",
        "\n",
        "for use_case in [\"research\", \"sharing\", \"local\", \"production\", \"portable\"]:\n",
        "    fmt, rationale = select_format(use_case, hardware=\"gpu\")\n",
        "    print(f\"{use_case}: {fmt}\")\n",
        "    print(f\"  {rationale[:85]}...\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Inference metrics (p50, p90, throughput)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "p50_latency_s: 1.2000\n",
            "p90_latency_s: 1.2600\n",
            "p50_ttft_s: 0.1000\n",
            "p90_ttft_s: 0.1060\n",
            "throughput_sustained_tok_s: 55.1724\n"
          ]
        }
      ],
      "source": [
        "from src.inference.metrics import compute_metrics\n",
        "\n",
        "total_latencies = [1.2, 1.1, 1.3, 1.0, 1.2]\n",
        "first_token_latencies = [0.1, 0.09, 0.11, 0.1, 0.1]\n",
        "token_counts = [64, 64, 64, 64, 64]\n",
        "\n",
        "metrics = compute_metrics(\n",
        "    total_latencies=total_latencies,\n",
        "    first_token_latencies=first_token_latencies,\n",
        "    token_counts=token_counts,\n",
        ")\n",
        "for k, v in metrics.items():\n",
        "    print(f\"{k}: {v:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Inference pipeline with TinyLlama (no Hugging Face login)\n",
        "\n",
        "Uses TinyLlama 1.1B, a public model. For Gemma, add `from huggingface_hub import login; login()` first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading weights: 100%|██████████| 201/201 [00:03<00:00, 51.98it/s, Materializing param=model.norm.weight]                              \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What was the peak brake pressure in vehicle V001? \n",
            "<|assistant|>\n",
            "The peak brake pressure in vehicle V001 was 120 psi (8.2\n"
          ]
        }
      ],
      "source": [
        "# Inference pipeline (requires torch; use Colab or Python 3.10-3.12 if local)\n",
        "try:\n",
        "    import torch\n",
        "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "    from src.inference.pipeline import InferencePipeline\n",
        "\n",
        "    model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "    )\n",
        "    model = model.to(device)\n",
        "\n",
        "    pipe = InferencePipeline(model, tokenizer, device=device, max_new_tokens=64)\n",
        "    out = pipe.generate([\"What was the peak brake pressure in vehicle V001?\"], max_new_tokens=32)\n",
        "    print(out[0])\n",
        "except RuntimeError as e:\n",
        "    if \"TORCH_LIBRARY\" in str(e) or \"prims\" in str(e):\n",
        "        print(\"PyTorch has compatibility issues with Python 3.13.\")\n",
        "        print(\"Run this cell in Colab (colab.research.google.com) or use Python 3.10-3.12 locally.\")\n",
        "    else:\n",
        "        raise"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
