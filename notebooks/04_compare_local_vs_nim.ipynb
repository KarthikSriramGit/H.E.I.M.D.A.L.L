{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Compare Local (Gemma 2 2B) vs NIM (Llama 3 8B)\n",
        "\n",
        "Runs the **same queries** through both pipelines and plots comparison: latency, response length, and side-by-side answers.\n",
        "\n",
        "**Requirements:** GPU, HF_TOKEN, NIM_BASE_URL (Colab Secrets), NIM running on GKE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/KarthikSriramGit/Project-Insight/blob/main/notebooks/04_compare_local_vs_nim.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Colab setup\n",
        "try:\n",
        "    import google.colab\n",
        "    get_ipython().system(\"git clone -q https://github.com/KarthikSriramGit/Project-Insight.git\")\n",
        "    get_ipython().run_line_magic(\"cd\", \"Project-Insight\")\n",
        "    get_ipython().system(\"pip install -q -r requirements.txt\")\n",
        "except Exception:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, time, subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "ROOT = Path(\".\").resolve()\n",
        "if str(ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(ROOT))\n",
        "\n",
        "data_path = ROOT / \"data\" / \"synthetic\" / \"fleet_telemetry.parquet\"\n",
        "if not data_path.exists():\n",
        "    subprocess.run([\"python\", \"data/synthetic/generate_telemetry.py\", \"--rows\", \"100000\", \"--output-dir\", \"data/synthetic\", \"--format\", \"parquet\"], check=True, cwd=str(ROOT))\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    NIM_BASE_URL = userdata.get(\"NIM_BASE_URL\")\n",
        "    os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
        "except Exception:\n",
        "    NIM_BASE_URL = os.environ.get(\"NIM_BASE_URL\", \"http://YOUR_IP:8000\")\n",
        "\n",
        "from src.query.engine import TelemetryQueryEngine\n",
        "from src.query.query_config import QUERY_CONFIG\n",
        "from src.query.prompts import SYSTEM_PROMPT, format_user_query\n",
        "\n",
        "engine = TelemetryQueryEngine(data_path=str(data_path), nim_base_url=NIM_BASE_URL, max_context_rows=500)\n",
        "print(\"Setup ready.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Run local model (Gemma 2 2B)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\".*torch_dtype.*\")\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from src.inference.pipeline import InferencePipeline\n",
        "\n",
        "model_id = \"google/gemma-2-2b-it\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16 if device==\"cuda\" else torch.float32)\n",
        "model = model.to(device)\n",
        "pipe = InferencePipeline(model, tokenizer, device=device, max_new_tokens=256)\n",
        "\n",
        "def clean_resp(txt, prompt=\"\"):\n",
        "    if prompt and txt.startswith(prompt): txt = txt[len(prompt):]\n",
        "    for x in [\"<start_of_turn>\", \"<end_of_turn>\", \"<bos>\", \"<eos>\", \"model\\n\", \"user\\n\"]: txt = txt.replace(x, \"\")\n",
        "    return txt.strip()\n",
        "\n",
        "results_02 = []\n",
        "for cfg in QUERY_CONFIG:\n",
        "    q = cfg[\"query\"]\n",
        "    if cfg.get(\"skip_data\"):\n",
        "        ctx = \"No telemetry — general knowledge question.\"\n",
        "    else:\n",
        "        df = engine.retrieve(vehicle_ids=cfg.get(\"vehicle_ids\"), sensor_type=cfg.get(\"sensor_type\"), brake_threshold=cfg.get(\"brake_threshold\"))\n",
        "        ctx = engine._data_to_context(df)\n",
        "    user_msg = f\"{SYSTEM_PROMPT}\\n\\n{format_user_query(q, ctx)}\"\n",
        "    chat = f\"<start_of_turn>user\\n{user_msg}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "    t0 = time.perf_counter()\n",
        "    out = pipe.generate([chat], max_new_tokens=256)\n",
        "    lat = time.perf_counter() - t0\n",
        "    ans = clean_resp(out[0], chat)\n",
        "    results_02.append({\"label\": cfg[\"label\"], \"query\": q, \"answer\": ans, \"latency_s\": lat, \"response_chars\": len(ans)})\n",
        "    print(f\"[Local] {cfg['label']}: {lat:.2f}s\")\n",
        "print(f\"Local total: {sum(r['latency_s'] for r in results_02):.2f}s\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Run NIM (Llama 3 8B on GKE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from src.deploy.nim_client import NIMClient\n",
        "\n",
        "nim_client = NIMClient(base_url=NIM_BASE_URL, max_tokens=256)\n",
        "results_03 = []\n",
        "\n",
        "for cfg in QUERY_CONFIG:\n",
        "    if cfg.get(\"skip_data\"):\n",
        "        user_msg = format_user_query(cfg[\"query\"], \"No telemetry — general knowledge question.\")\n",
        "        t0 = time.perf_counter()\n",
        "        ans = nim_client.ask(user_msg, system_context=SYSTEM_PROMPT)\n",
        "        lat = time.perf_counter() - t0\n",
        "    else:\n",
        "        t0 = time.perf_counter()\n",
        "        ans = engine.query(cfg[\"query\"], vehicle_ids=cfg.get(\"vehicle_ids\"), sensor_type=cfg.get(\"sensor_type\"), brake_threshold=cfg.get(\"brake_threshold\"))\n",
        "        lat = time.perf_counter() - t0\n",
        "    results_03.append({\"label\": cfg[\"label\"], \"query\": cfg[\"query\"], \"answer\": ans, \"latency_s\": lat, \"response_chars\": len(ans)})\n",
        "    print(f\"[NIM] {cfg['label']}: {lat:.2f}s\")\n",
        "print(f\"NIM total: {sum(r['latency_s'] for r in results_03):.2f}s\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Comparison plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "labels = [r[\"label\"] for r in results_02]\n",
        "x = np.arange(len(labels))\n",
        "w = 0.35\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "# Latency comparison\n",
        "ax1 = axes[0, 0]\n",
        "lat_02 = [r[\"latency_s\"] for r in results_02]\n",
        "lat_03 = [r[\"latency_s\"] for r in results_03]\n",
        "ax1.bar(x - w/2, lat_02, w, label=\"Local (Gemma 2 2B)\", color=\"steelblue\", alpha=0.8)\n",
        "ax1.bar(x + w/2, lat_03, w, label=\"NIM (Llama 3 8B)\", color=\"coral\", alpha=0.8)\n",
        "ax1.set_ylabel(\"Latency (s)\")\n",
        "ax1.set_title(\"Query latency: Local vs NIM\")\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
        "ax1.legend()\n",
        "\n",
        "# Response length comparison\n",
        "ax2 = axes[0, 1]\n",
        "ch_02 = [r[\"response_chars\"] for r in results_02]\n",
        "ch_03 = [r[\"response_chars\"] for r in results_03]\n",
        "ax2.bar(x - w/2, ch_02, w, label=\"Local\", color=\"steelblue\", alpha=0.8)\n",
        "ax2.bar(x + w/2, ch_03, w, label=\"NIM\", color=\"coral\", alpha=0.8)\n",
        "ax2.set_ylabel(\"Chars\")\n",
        "ax2.set_title(\"Response length\")\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
        "ax2.legend()\n",
        "\n",
        "# Summary: total time\n",
        "ax3 = axes[1, 0]\n",
        "ax3.bar([\"Local\", \"NIM\"], [sum(lat_02), sum(lat_03)], color=[\"steelblue\", \"coral\"], alpha=0.8)\n",
        "ax3.set_ylabel(\"Total time (s)\")\n",
        "ax3.set_title(\"Total inference time\")\n",
        "\n",
        "# Summary: avg latency\n",
        "ax4 = axes[1, 1]\n",
        "ax4.bar([\"Local\", \"NIM\"], [np.mean(lat_02), np.mean(lat_03)], color=[\"steelblue\", \"coral\"], alpha=0.8)\n",
        "ax4.set_ylabel(\"Avg latency (s)\")\n",
        "ax4.set_title(\"Average latency per query\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Insights summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "total_02 = sum(r[\"latency_s\"] for r in results_02)\n",
        "total_03 = sum(r[\"latency_s\"] for r in results_03)\n",
        "avg_02 = total_02 / len(results_02)\n",
        "avg_03 = total_03 / len(results_03)\n",
        "ratio = total_03 / total_02 if total_02 > 0 else 0\n",
        "\n",
        "print(\"Insights:\")\n",
        "print(f\"  • Local (Gemma 2 2B) total: {total_02:.2f}s | avg/query: {avg_02:.2f}s\")\n",
        "print(f\"  • NIM (Llama 3 8B) total:   {total_03:.2f}s | avg/query: {avg_03:.2f}s\")\n",
        "print(f\"  • NIM is {ratio:.2f}x {'faster' if ratio < 1 else 'slower'} than local (total time)\")\n",
        "print(f\"  • Local avg response length: {sum(r['response_chars'] for r in results_02)/len(results_02):.0f} chars\")\n",
        "print(f\"  • NIM avg response length:   {sum(r['response_chars'] for r in results_03)/len(results_03):.0f} chars\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Side-by-side answers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for i, lbl in enumerate(labels):\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"Query: {lbl}\")\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"Local: {results_02[i]['answer'][:300]}{'...' if len(results_02[i]['answer'])>300 else ''}\")\n",
        "    print(f\"NIM:   {results_03[i]['answer'][:300]}{'...' if len(results_03[i]['answer'])>300 else ''}\")\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}